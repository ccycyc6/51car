{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-10 19:15:05--  http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531887/train_sample.zip\n",
      "Resolving tianchi-competition.oss-cn-hangzhou.aliyuncs.com (tianchi-competition.oss-cn-hangzhou.aliyuncs.com)... 183.131.227.248\n",
      "Connecting to tianchi-competition.oss-cn-hangzhou.aliyuncs.com (tianchi-competition.oss-cn-hangzhou.aliyuncs.com)|183.131.227.248|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 540689175 (516M) [application/zip]\n",
      "Saving to: ‘train_sample.zip’\n",
      "\n",
      "train_sample.zip    100%[===================>] 515.64M  3.43MB/s    in 1m 47s  \n",
      "\n",
      "2025-05-10 19:16:53 (4.81 MB/s) - ‘train_sample.zip’ saved [540689175/540689175]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531887/train_sample.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq train_sample.zip\n",
    "!\\rm train_sample.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d956e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-10 19:19:06--  http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531887/test_a.zip\n",
      "Resolving tianchi-competition.oss-cn-hangzhou.aliyuncs.com (tianchi-competition.oss-cn-hangzhou.aliyuncs.com)... 183.131.227.248\n",
      "Connecting to tianchi-competition.oss-cn-hangzhou.aliyuncs.com (tianchi-competition.oss-cn-hangzhou.aliyuncs.com)|183.131.227.248|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1092637852 (1.0G) [application/zip]\n",
      "Saving to: ‘test_a.zip’\n",
      "\n",
      "test_a.zip          100%[===================>]   1.02G  5.46MB/s    in 4m 8s   \n",
      "\n",
      "2025-05-10 19:23:15 (4.20 MB/s) - ‘test_a.zip’ saved [1092637852/1092637852]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531887/test_a.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq test_a.zip\n",
    "!\\rm test_a.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530d3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -i https://mirrors.aliyun.com/pypi/simple pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04729356-175f-4fb2-9986-d553fcb544bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -i https://mirrors.aliyun.com/pypi/simple scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27533bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: resampy in /opt/conda/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from resampy) (1.26.0)\n",
      "Requirement already satisfied: numba>=0.53 in /opt/conda/lib/python3.10/site-packages (from resampy) (0.61.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.53->resampy) (0.44.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -i https://mirrors.aliyun.com/pypi/simple resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f8d3e-e4c6-4fb5-a1ca-2ebd7973668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.7.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from lazy_loader>=0.1->librosa) (23.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: resampy in /opt/conda/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from resampy) (1.26.0)\n",
      "Requirement already satisfied: numba>=0.53 in /opt/conda/lib/python3.10/site-packages (from resampy) (0.61.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.53->resampy) (0.44.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install librosa -i https://pypi.tuna.tsinghua.edu.cn/simple --default-timeout=100\n",
    "!pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa5d51-2c6f-4161-a61f-1bf515372668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要的PyTorch库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b4fa2-e418-4ef8-b4b3-ab20b9adaa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39db8509-f430-4fed-86f7-4697f4999e12",
   "metadata": {},
   "source": [
    "# 特征提取以及数据集的建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6e41b-13d6-40ec-badf-6c18832e3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 初始化特征和标签列表\n",
    "feature = []\n",
    "label = []\n",
    "\n",
    "# 建立类别标签，不同类别对应不同的数字\n",
    "label_dict = {'aloe': 0, 'burger': 1, 'cabbage': 2, 'candied_fruits': 3, 'carrots': 4, 'chips': 5,\n",
    "              'chocolate': 6, 'drinks': 7, 'fries': 8, 'grapes': 9, 'gummies': 10, 'ice-cream': 11,\n",
    "              'jelly': 12, 'noodles': 13, 'pickles': 14, 'pizza': 15, 'ribs': 16, 'salmon': 17,\n",
    "              'soup': 18, 'wings': 19}\n",
    "\n",
    "# 创建一个反向字典，用于从数字映射回类别名称\n",
    "label_dict_inv = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "# 定义特征提取函数\n",
    "def extract_features(parent_dir, sub_dirs, max_file=10, file_ext=\"*.wav\", augment=False):\n",
    "    \"\"\"\n",
    "    从音频文件中提取特征和标签。\n",
    "\n",
    "    参数:\n",
    "    - parent_dir: 数据集的父目录\n",
    "    - sub_dirs: 子目录列表，每个子目录对应一个类别\n",
    "    - max_file: 每个类别最多处理的文件数量\n",
    "    - file_ext: 文件扩展名，默认为 \"*.wav\"\n",
    "\n",
    "    返回:\n",
    "    - feature: 提取的特征列表\n",
    "    - label: 对应的标签列表\n",
    "    \"\"\"\n",
    "    c = 0  # 初始化计数器\n",
    "    label, feature = [], []  # 初始化标签和特征列表\n",
    "\n",
    "    # 遍历每个子目录\n",
    "    for sub_dir in sub_dirs:\n",
    "        # 遍历子目录中的音频文件，最多处理 max_file 个文件\n",
    "        for fn in tqdm(glob.glob(os.path.join(parent_dir, sub_dir, file_ext))[:max_file]):\n",
    "            # 获取文件的类别标签（子目录名称）\n",
    "            label_name = fn.split('/')[-2]\n",
    "            # 将类别标签转换为数字，并添加到标签列表中\n",
    "            label.extend([label_dict[label_name]])\n",
    "            # 加载音频文件\n",
    "            X, sample_rate = librosa.load(fn, res_type='kaiser_fast')\n",
    "            # 数据增强\n",
    "            if augment:\n",
    "                X = augment_audio(X, sample_rate)\n",
    "            # 计算梅尔频谱（mel spectrogram），并取其均值作为特征\n",
    "            mels = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "            # 将特征添加到特征列表中\n",
    "            feature.extend([mels])\n",
    "    \n",
    "    # 返回特征和标签\n",
    "    return [feature, label]\n",
    "\n",
    "# 数据增强函数\n",
    "def augment_audio(X, sample_rate):\n",
    "    # 随机添加高斯噪声\n",
    "    if random.random() < 0.5:\n",
    "        noise = np.random.randn(len(X))\n",
    "        X = X + 0.005 * noise\n",
    "    # 随机时移\n",
    "    if random.random() < 0.5:\n",
    "        shift = np.random.randint(sample_rate // 10)\n",
    "        X = np.roll(X, shift)\n",
    "    # 随机音量扰动\n",
    "    if random.random() < 0.5:\n",
    "        X = X * (0.8 + 0.4 * np.random.rand())\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b60718-3c06-423e-97a6-c42144f03701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [02:12<00:00,  2.94s/it]\n",
      "100%|██████████| 64/64 [02:58<00:00,  2.79s/it]\n",
      "100%|██████████| 48/48 [01:51<00:00,  2.31s/it]\n",
      "100%|██████████| 74/74 [03:11<00:00,  2.59s/it]\n",
      "100%|██████████| 49/49 [02:21<00:00,  2.89s/it]\n",
      "100%|██████████| 57/57 [02:23<00:00,  2.52s/it]\n",
      "100%|██████████| 27/27 [01:07<00:00,  2.51s/it]\n",
      "100%|██████████| 27/27 [01:10<00:00,  2.61s/it]\n",
      "100%|██████████| 57/57 [02:20<00:00,  2.47s/it]\n",
      "100%|██████████| 61/61 [02:10<00:00,  2.13s/it]\n",
      "100%|██████████| 65/65 [02:19<00:00,  2.15s/it]\n",
      "100%|██████████| 69/69 [02:59<00:00,  2.60s/it]\n",
      "100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n",
      " 76%|███████▌  | 25/33 [00:59<00:18,  2.30s/it]"
     ]
    }
   ],
   "source": [
    "# 加载训练数据\n",
    "# 定义训练数据的父目录\n",
    "parent_dir = './train_sample/'\n",
    "\n",
    "# 定义保存目录（当前未使用）\n",
    "save_dir = \"./\"\n",
    "\n",
    "# 定义类别名称数组，每个类别对应一个子目录\n",
    "folds = sub_dirs = np.array(['aloe', 'burger', 'cabbage', 'candied_fruits',\n",
    "                             'carrots', 'chips', 'chocolate', 'drinks', 'fries',\n",
    "                             'grapes', 'gummies', 'ice-cream', 'jelly', 'noodles', 'pickles',\n",
    "                             'pizza', 'ribs', 'salmon', 'soup', 'wings'])\n",
    "\n",
    "# 调用特征提取函数，从音频文件中提取特征和标签\n",
    "# 参数包括父目录、子目录列表以及每个类别最多处理的文件数量\n",
    "# 数据增强开关\n",
    "AUGMENT = True\n",
    "X, Y = extract_features(parent_dir, sub_dirs, max_file=100, augment=AUGMENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872bee5d-8453-4dc6-bdb3-1e93581a57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分割，将数据集划分为训练集和测试集\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1, stratify=Y)\n",
    "\n",
    "# 转换为NumPy数组，确保数据格式一致\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# 调整数据形状为 (N, 1, 16, 8)，以适配卷积神经网络的输入格式\n",
    "X_train = X_train.reshape(-1, 16, 8, 1).transpose(0, 3, 1, 2)\n",
    "X_test = X_test.reshape(-1, 16, 8, 1).transpose(0, 3, 1, 2)\n",
    "\n",
    "# 转换为PyTorch张量，创建训练集和测试集的数据集对象\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(Y_train))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc3a3f-e142-464b-91c2-3c6220965378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载器\n",
    "# 定义批量大小\n",
    "batch_size = 64\n",
    "\n",
    "# 创建训练数据加载器，支持随机打乱数据\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 创建测试数据加载器，不打乱数据\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ceb88-8887-4a14-935a-d25b78036e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义音频分类器模型\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 定义卷积层部分\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # 第一个卷积层，输入通道为1，输出通道为64，卷积核大小为3x3，使用padding=1保持输入输出尺寸一致\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            # 批归一化层，规范化卷积层输出，加速收敛\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 使用Tanh激活函数\n",
    "            nn.Tanh(),\n",
    "            # 最大池化层，池化窗口大小为2x2\n",
    "            nn.MaxPool2d(2),\n",
    "            # 第二个卷积层，输入通道为64，输出通道为128，卷积核大小为3x3，使用padding=1\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            # 批归一化层\n",
    "            nn.BatchNorm2d(128),\n",
    "            # 使用Tanh激活函数\n",
    "            nn.Tanh(),\n",
    "            # 最大池化层，池化窗口大小为2x2\n",
    "            nn.MaxPool2d(2),\n",
    "            # Dropout层，防止过拟合，丢弃率为0.3\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # 定义全连接层部分\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            # 展平层，将多维张量展平为一维\n",
    "            nn.Flatten(),\n",
    "            # 全连接层，输入大小为128 * 4 * 2（经过卷积和池化后的特征图大小），输出大小为1024\n",
    "            nn.Linear(128 * 4 * 2, 1024),\n",
    "            # 使用Tanh激活函数\n",
    "            nn.Tanh(),\n",
    "            # Dropout层，丢弃率为0.3\n",
    "            nn.Dropout(0.3),\n",
    "            # 全连接层，输入大小为1024，输出大小为20（对应20个类别）\n",
    "            nn.Linear(1024, 20)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播，先通过卷积层部分\n",
    "        x = self.conv_layers(x)\n",
    "        # 再通过全连接层部分\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "# DenseNet和ResNet实现\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.BatchNorm2d(in_channels + i * growth_rate),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels + i * growth_rate, growth_rate, 3, padding=1, bias=False)\n",
    "            ))\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            out = layer(torch.cat(features, 1))\n",
    "            features.append(out)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "class DenseNetSmall(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.block1 = DenseBlock(32, 16, 3)\n",
    "        self.trans1 = nn.Conv2d(32+16*3, 64, 1)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.block2 = DenseBlock(64, 16, 3)\n",
    "        self.trans2 = nn.Conv2d(64+16*3, 128, 1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(128*4*2, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.block1(x)\n",
    "        x = F.relu(self.trans1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.block2(x)\n",
    "        x = F.relu(self.trans2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNetSmall(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.layer1 = BasicBlock(32, 64, stride=2)\n",
    "        self.layer2 = BasicBlock(64, 128, stride=2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(128*4*2, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# 原始模型增加正则化\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 2, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 20)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3bef8-abc8-4553-8196-e63558894984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[HAMI-core Msg(1781:140142402833280:libvgpu.c:859)]: Initialized\n"
     ]
    }
   ],
   "source": [
    "# 选择模型类型：'base', 'densenet', 'resnet'\n",
    "MODEL_TYPE = 'densenet'  # 可选 'base', 'densenet', 'resnet'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if MODEL_TYPE == 'densenet':\n",
    "    model = DenseNetSmall(num_classes=20).to(device)\n",
    "elif MODEL_TYPE == 'resnet':\n",
    "    model = ResNetSmall(num_classes=20).to(device)\n",
    "else:\n",
    "    model = AudioClassifier().to(device)\n",
    "\n",
    "# L2正则化\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940de13-02f6-4ede-bc8b-00c091f65f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioClassifier(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Tanh()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=1024, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 打印模型结构\n",
    "# 通过打印模型，可以查看模型的各层结构和参数信息\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8544bd-9d06-4bdd-aaaa-79f0b8e311ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, \n",
    "                num_epochs=100, validate_every=10, show_progress_every=1):\n",
    "    \"\"\"\n",
    "    训练模型并定期进行验证。\n",
    "\n",
    "    参数:\n",
    "    - model: 要训练的PyTorch模型。\n",
    "    - train_loader: 训练数据集的DataLoader。\n",
    "    - test_loader: 验证数据集的DataLoader。\n",
    "    - criterion: 损失函数，用于计算模型预测与真实标签之间的误差。\n",
    "    - optimizer: 优化算法，用于更新模型参数。\n",
    "    - num_epochs: 训练的总轮数（默认值: 100）。\n",
    "    - validate_every: 每隔多少轮进行一次验证（默认值: 10）。\n",
    "    - show_progress_every: 每隔多少轮显示一次进度条（默认值: 1）。\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        # 设置模型为训练模式，启用dropout和batch normalization等训练特性\n",
    "        model.train()\n",
    "        \n",
    "        # 初始化训练损失、正确预测计数和样本总数\n",
    "        train_loss = 0.0  # 累计训练损失\n",
    "        train_correct = 0  # 累计正确预测的样本数\n",
    "        train_total = 0  # 累计训练样本总数\n",
    "\n",
    "        # 控制是否显示进度条\n",
    "        loader = train_loader if (epoch+1) % show_progress_every == 0 else train_loader\n",
    "        disable_progress = not ((epoch+1) % show_progress_every == 0)\n",
    "\n",
    "        # 遍历训练数据集的每个批次\n",
    "        for inputs, labels in tqdm(loader, \n",
    "                                 desc=f\"Epoch {epoch+1}/{num_epochs}\", \n",
    "                                 unit=\"batch\",\n",
    "                                 disable=disable_progress):\n",
    "            # 将输入和标签移动到设备（CPU/GPU）\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 清零优化器的梯度，避免梯度累积\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播计算输出\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 计算损失，衡量预测与真实标签的差距\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播计算梯度\n",
    "            loss.backward()\n",
    "            \n",
    "            # 使用优化器更新模型参数\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 累计训练损失\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # 获取预测结果并统计正确预测的数量\n",
    "            _, predicted = torch.max(outputs.data, 1)  # 获取每个样本的预测类别\n",
    "            train_total += labels.size(0)  # 累计样本总数\n",
    "            train_correct += (predicted == labels).sum().item()  # 累计正确预测的样本数\n",
    "        \n",
    "        # 计算平均训练损失和准确率\n",
    "        train_loss = train_loss / len(train_loader)  # 平均训练损失\n",
    "        train_acc = 100 * train_correct / train_total  # 训练准确率（百分比）\n",
    "        \n",
    "        # 验证阶段（根据指定频率）\n",
    "        if (epoch + 1) % validate_every == 0 or epoch == num_epochs - 1:\n",
    "            # 设置模型为评估模式，禁用dropout等训练特性\n",
    "            model.eval()\n",
    "            \n",
    "            # 初始化验证损失、正确预测计数和样本总数\n",
    "            val_loss = 0.0  # 累计验证损失\n",
    "            val_correct = 0  # 累计正确预测的样本数\n",
    "            val_total = 0  # 累计验证样本总数\n",
    "            \n",
    "            # 禁用梯度计算以加速验证过程\n",
    "            with torch.no_grad():\n",
    "                # 遍历验证数据集的每个批次\n",
    "                for inputs, labels in test_loader:\n",
    "                    # 将输入和标签移动到设备（CPU/GPU）\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                    # 前向传播计算输出\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    # 计算损失\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # 获取预测结果并统计正确预测的数量\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)  # 累计样本总数\n",
    "                    val_correct += (predicted == labels).sum().item()  # 累计正确预测的样本数\n",
    "            \n",
    "            # 计算验证准确率\n",
    "            val_acc = 100 * val_correct / val_total  # 验证准确率（百分比）\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs} - Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            # 非验证轮次仅显示训练准确率\n",
    "            if not disable_progress:\n",
    "                print(f\"\\nEpoch {epoch+1}/{num_epochs} - Train Acc: {train_acc:.2f}%\")\n",
    "    \n",
    "    # 训练完成后打印最终结果\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Final Train Accuracy: {train_acc:.2f}%\")\n",
    "    \n",
    "    # 确保最后一个轮次进行验证（如果未在最后一轮验证）\n",
    "    # 如果最后一个轮次未进行验证，则在训练结束后进行一次最终验证\n",
    "    if (num_epochs - 1) % validate_every != 0:\n",
    "        # 设置模型为评估模式，禁用dropout等训练特性\n",
    "        model.eval()\n",
    "        \n",
    "        # 初始化验证损失、正确预测计数和样本总数\n",
    "        val_loss = 0.0  # 累计验证损失\n",
    "        val_correct = 0  # 累计正确预测的样本数\n",
    "        val_total = 0  # 累计验证样本总数\n",
    "        \n",
    "        # 禁用梯度计算以加速验证过程\n",
    "        with torch.no_grad():\n",
    "            # 遍历验证数据集的每个批次\n",
    "            for inputs, labels in test_loader:\n",
    "                # 将输入和标签移动到设备（CPU/GPU）\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # 前向传播计算输出\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # 获取预测结果并统计正确预测的数量\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)  # 累计样本总数\n",
    "                val_correct += (predicted == labels).sum().item()  # 累计正确预测的样本数\n",
    "        \n",
    "        # 计算最终验证准确率\n",
    "        val_acc = 100 * val_correct / val_total  # 验证准确率（百分比）\n",
    "        print(f\"Final Validation Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c3f8c-361b-4a97-8d3d-e607b6c6bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/550: 100%|██████████| 13/13 [00:00<00:00, 247.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55/550\n",
      "Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Val Loss: 7.6921 | Val Acc: 38.50%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/550: 100%|██████████| 13/13 [00:00<00:00, 271.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 110/550\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Val Loss: 7.6659 | Val Acc: 38.50%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165/550: 100%|██████████| 13/13 [00:00<00:00, 270.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 165/550\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Val Loss: 7.6766 | Val Acc: 39.00%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 220/550: 100%|██████████| 13/13 [00:00<00:00, 271.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 220/550\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Val Loss: 7.7035 | Val Acc: 40.00%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 275/550: 100%|██████████| 13/13 [00:00<00:00, 247.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 275/550\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Val Loss: 7.7544 | Val Acc: 39.00%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 330/550: 100%|██████████| 13/13 [00:00<00:00, 153.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 330/550\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Val Loss: 7.7732 | Val Acc: 39.00%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 385/550: 100%|██████████| 13/13 [00:00<00:00, 249.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 385/550\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Val Loss: 7.8913 | Val Acc: 40.00%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 440/550: 100%|██████████| 13/13 [00:00<00:00, 258.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 440/550\n",
      "Train Loss: 0.0007 | Train Acc: 100.00%\n",
      "Val Loss: 7.7391 | Val Acc: 39.00%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 495/550: 100%|██████████| 13/13 [00:00<00:00, 269.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 495/550\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Val Loss: 7.8197 | Val Acc: 40.00%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 550/550: 100%|██████████| 13/13 [00:00<00:00, 269.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 550/550\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Val Loss: 7.8078 | Val Acc: 39.50%\n",
      "--------------------------------------------------\n",
      "\n",
      "Training completed!\n",
      "Final Train Accuracy: 100.00%\n",
      "Final Validation Accuracy: 39.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "# 调用 train_model 函数，开始训练音频分类器模型\n",
    "# 参数说明：\n",
    "# - model: 要训练的模型实例\n",
    "# - train_loader: 训练数据的 DataLoader\n",
    "# - test_loader: 测试数据的 DataLoader，用于验证模型性能\n",
    "# - criterion: 损失函数，用于计算预测误差\n",
    "# - optimizer: 优化器，用于更新模型参数\n",
    "# - num_epochs: 训练的总轮数，这里设置为 500\n",
    "# - validate_every: 每隔 50 个 epoch 进行一次验证\n",
    "# - show_progress_every: 每隔 50 个 epoch 显示一次训练进度\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, \n",
    "           num_epochs=500, validate_every=50, show_progress_every=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50741c4-5595-4edd-9549-f9be398cbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_features(test_dir, max_file=None, file_ext=\"*.wav\"):\n",
    "    \"\"\"\n",
    "    从测试音频文件中提取特征。\n",
    "\n",
    "    参数:\n",
    "    - test_dir: 测试数据的目录路径。\n",
    "    - max_file: 最多处理的文件数量（默认值: None，表示处理所有文件）。\n",
    "    - file_ext: 文件扩展名，默认为 \"*.wav\"。\n",
    "\n",
    "    返回:\n",
    "    - features: 提取的特征，作为 PyTorch FloatTensor。\n",
    "    - file_list: 处理的文件路径列表。\n",
    "    \"\"\"\n",
    "    features = []  # 初始化特征列表\n",
    "    file_list = glob.glob(os.path.join(test_dir, file_ext))  # 获取指定目录下的所有音频文件\n",
    "    if max_file is not None:  # 如果指定了最大文件数量\n",
    "        file_list = file_list[:max_file]  # 截取前 max_file 个文件\n",
    "    \n",
    "    # 遍历文件列表，提取每个文件的特征\n",
    "    for fn in tqdm(file_list):\n",
    "        # 加载音频文件\n",
    "        X, sample_rate = librosa.load(fn, res_type='kaiser_fast')\n",
    "        # 计算梅尔频谱（mel spectrogram），并取其均值作为特征\n",
    "        mels = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        features.append(mels)  # 将特征添加到特征列表中\n",
    "    \n",
    "    features = np.array(features)  # 转换为 NumPy 数组\n",
    "    # 返回特征和文件列表\n",
    "    return torch.FloatTensor(features), file_list\n",
    "\n",
    "def predict(model, test_features):\n",
    "    \"\"\"\n",
    "    使用模型对测试特征进行预测。\n",
    "\n",
    "    参数:\n",
    "    - model: 训练好的 PyTorch 模型。\n",
    "    - test_features: 测试数据的特征，作为 PyTorch 张量。\n",
    "\n",
    "    返回:\n",
    "    - predictions: 预测的类别名称列表。\n",
    "    \"\"\"\n",
    "    model.eval()  # 设置模型为评估模式，禁用 dropout 等训练特性\n",
    "    with torch.no_grad():  # 禁用梯度计算以加速预测过程\n",
    "        test_features = test_features.to(device)  # 将特征移动到设备（CPU/GPU）\n",
    "        outputs = model(test_features)  # 前向传播计算输出\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取每个样本的预测类别\n",
    "        # 将预测的类别数字映射回类别名称\n",
    "        return [label_dict_inv[p.item()] for p in predicted]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f8df7-9f56-4e2c-b90d-04fef0e35597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [1:04:20<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "test_features, file_list = extract_test_features('./test_a/')\n",
    "# 调整测试特征的形状以适配模型输入\n",
    "# 这里将特征调整为 (N, 1, 16, 8) 的形状，其中 N 是样本数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd80945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整测试特征的形状为 (N, 1, 16, 8)，其中 N 是样本数量\n",
    "# 1 表示单通道（灰度图像），16 和 8 是特征的高和宽\n",
    "# 这种形状适配卷积神经网络的输入格式\n",
    "test_features = test_features.view(-1, 1, 16, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9d881-2162-4868-8d4b-0b5cefe035f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练好的模型对测试特征进行预测\n",
    "# 参数:\n",
    "# - model: 训练好的音频分类器模型\n",
    "# - test_features: 提取并调整形状后的测试数据特征\n",
    "# 返回:\n",
    "# - predictions: 测试数据的预测类别名称列表\n",
    "predictions = predict(model, test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53dc4cc-928a-42a2-8435-2ae568b7c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将预测结果赋值给变量 preds\n",
    "preds = predictions\n",
    "\n",
    "# 获取测试文件的路径列表\n",
    "path = glob.glob('./test_a/*.wav')\n",
    "\n",
    "# 创建一个 DataFrame，将文件路径和预测标签对应起来\n",
    "result = pd.DataFrame({'name': path, 'label': preds})\n",
    "\n",
    "# 提取文件名（去掉路径部分），并更新到 'name' 列\n",
    "result['name'] = result['name'].apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "# 将结果保存为 CSV 文件，文件名为 'submit.csv'，不包含索引列\n",
    "result.to_csv('submit.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e8347-b263-47bb-9670-af8d601c684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# 统计测试数据目录中以 .wav 为扩展名的音频文件数量\n",
    "!ls ./test_a/*.wav | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6d3f8-58a9-482d-a11a-baa8ac576de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001 submit.csv\n"
     ]
    }
   ],
   "source": [
    "# 使用 wc -l 命令统计 submit.csv 文件中的行数\n",
    "# 这可以用来检查提交文件中包含的记录数量\n",
    "!wc -l submit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb4c8a-4645-4e53-b569-b1cc2160ab86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
